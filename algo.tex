\section{MBFQ Algorithm}
\label{sec:algorithm}

\begin{table}[t]
{\footnotesize
\begin{tabular}{ | c | p{6cm} | }
  \hline
  $C$ & The link capacity to share among VMs \\ \hline
  $AvailBw_{All}$ & Bandwidth available, initialized to $C$. \\ \hline
  $W_{All}$ & Total weight of VMs that need bandwidth. \\ \hline
  $VmCount_{All}$ & Number of VMs that need bandwidth. \\ \hline 
  $SR_{i}$ & The measured send rate of the VM. \\ \hline
  $AR_{i}$ & The current allocated rate for the VM. \\ \hline
  $TR_{i}$ & The target rate for the VM, based on its $SR_{i}$. \\ \hline
  $RU_{i}$ & Consecutive iterations VM needs bandwidth. \\ \hline
  $MG_{i}$ & The VM's minimum bandwidth guarantee. \\ \hline
  $W_{i}$ & The VM's weight, the ratio of $MG_{i}$ to $C$. \\ \hline
  $NR_{i}$ & New rate being allocated to the VM.   \\ \hline
  $BelowTR_{i}$ & Whether $NR_{i}$ is less than $TR_{i}$. \\ \hline
\end{tabular}
}
\vspace{-1em}
\caption{MBFQ notation}
\vspace{-1em}
\label{tab:mbfq_init}
\end{table}

\begin{figure}[t]
{\footnotesize
\begin{algorithmic}[1]
\State $VmCount_{All} \gets 0$
\State $W_{All} \gets 0$
\State $AvailBw_{All} \gets C$
\For{(each $VM$ sharing the link capacity)}
\State $SR_{i} \gets AverageSendRateInLastTSeconds$
\If {($AR_{i} = Disabled$)}
\State $TR_{i} \gets 1.1 \times SR_{i}$
\ElsIf {($SR_{i} < 0.85 \times AR_{i}$ for the last 500 msec)}
\State $TR_{i} \gets 1.1 \times SR_{i}$ 
\State $RU_{i} \gets max(0, RU_{i}-1)$
\ElsIf {($SR_{i} > 0.95 \times AR_{i}$)}
\State $RU_{i} \gets min(3, RU_{i} + 1)$
\If {($RU_{i}$ = 1)}
\State $TR_{i} \gets min(1.2 \times AR_{i}, AR_{i} + 0.1 \times C$)
\ElsIf {($RU_{i}$ = 2)}
\State $TR_{i} \gets min(1.5 \times AR_{i}, AR_{i} + 0.1 \times C)$
\ElsIf {($RU_{i}$ = 3)}
\State $TR_{i} \gets max(2 \times AR_{i}, MG_{i})$
\EndIf
\Else
\State $TR_{i} \gets AR_{i}$
\State $RU_{i} \gets max(0, RU_{i} - 1)$
\EndIf
\\
\State $TR_{i} \gets max(TR_{i}, 10Mbps)$
\State $NR_{i} \gets min(TR_{i}, MG_{i})$
\\
\If {($NR_{i} < TR_{i}$)}
\State $BelowTR_{i} \gets true$
\State $W_{All} \gets W_{All} + W_{i}$
\State $VmCount_{All} \gets VmCount_{All} + 1$
\Else
\State $BelowTR_{i} \gets false$
\EndIf
\\
\State $AvailBw_{All} \gets AvailBw_{All} - NR_{i}$
\EndFor
\end{algorithmic}
}
\vspace{-0.5em}
\caption{MBFQ Phase 1}
\label{fig:mbfq_p1}
\vspace{-2em}
\end{figure}

\begin{figure}[t]
{\footnotesize
\begin{algorithmic}[1]
\setalglineno{33} 
\While{($AvailBw_{All} > 0$ and $VmCount_{All} \ne 0$)}
\For{(each $VM$ sharing the network adapter)}
\If {($BelowTR_{i} = true$)}
\State $FairShare_{i} \gets AvailBw_{All} \times W_{i} \div W_{All}$
\State $NR_{i} \gets NR_{i} + FairShare_{i}$
\EndIf
\\
\If {($NR_{i} \geq TR_{i}$)}
\State $AvailBw_{All} \gets AvailBw_{All} + (NR_{i} - TR_{i})$
\State $NR_{i} = TR_{i}$
\State $BelowTR_{i} \gets false$
\State $W_{All} \gets W_{All} - W_{i}$
\State $VmCount_{All} \gets VmCount_{All} - 1$
\EndIf
\EndFor
\EndWhile
\end{algorithmic}
}
\vspace{-0.5em}
\caption{MBFQ Phase 2}
\label{fig:mbfq_p2}
\vspace{-1em}
\end{figure}

Table~\ref{tab:mbfq_init} shows the notation we use in this section.  The MBFQ
algorithm runs every $T$ seconds. Rates are computed in two phases.  First, we
compute an ideal target rate ($TR$) for each VM, by comparing its measured send
rate ($SR$) to the allocated rate ($AR$). $TR$ is a proxy for what the VM would
desire if it were not subject to constraints like link bandwidth, and sharing
with other VMs.  In the second phase, we adjust the $TR$ to ensure that the link
is not oversubscribed and rates are allocated according to weights. These
adjusted target rates then become the allocated rates.

{\bf Phase 1: Computing target rates:} 
The pseudocode for phase 1 is shown in Figure~\ref{fig:mbfq_p1}. We calculate
a new target rate for every VM, based on its recently-measured send rate ($SR$).
Intuitively, if a VM is not fully using its allocated rate, we should offer the
residual bandwidth to other VMs.  On the other hand, if the VM is indeed fully
using its allocated rate, it may be able to use additional bandwidth. 

We assume that a VM is fully utilizing is allocated bandwidth if $SR \geq
0.95*AR$ (line 9). The 5\% margin allows for small, unavoidable variations in
the sending rate. To increase the bandwidth allocated to such a VM we need to
strike a balance between two competing goals. First, we must minimize
``waste''-- the VM may not be able to use the additional bandwidth that we give
it.  Second, customers must be able to quickly ramp up to the bandwidth that
they have paid for, if they are backlogged.

Our approach works as follows (lines 10 - 16).  If, for a given VM, $SR \geq
0.95*AR$, we set $TR = 1.2*AR$.  If the VM qualifies again in the immediate next
round, we set $TR = 1.5*AR$, and if it qualifies again, we set $TR = \max(2*AR,
MG)$ -- i.e.  if needed, we let it go to full minimum guaranteed bandwidth (or
higher).  Thus, we are conservative in the first two rounds, but a customer with
substantial pending demand is guaranteed to reach the minimum guaranteed
bandwidth in three time intervals or less. This staged allocation increment is a balance
between granting the VM its bandwidth guarantee as quickly as possible, and minimizing
unused bandwidth (i.e. maximizing work-conserving property).

If, on the other hand, the VM is using less than 85\% of its allocated
bandwidth, i.e. $SR \leq 0.85*AR$ for five consecutive intervals (i.e. 500 milliseconds),
we reduce the bandwidth allocated to this VM by setting $TR = 1.1*SR$. 

{\bf Phase 2:  Preventing Congestion and Enforcing Fair Sharing:} The pseudocode
for phase 2 is shown in Figure~\ref{fig:mbfq_p2}. Our goal is to adjust the
allocated rates so that the link is not oversubscribed, and any leftover bandwidth
is allocated in proportion to the VMs' weights.

We start by initializing each VM's allocated rate ($AR$) to be the minimum of its 
target rate ($TR$)
and its guaranteed rate ($MG$) (line 22  in Figure~\ref{fig:mbfq_p1}) .  
This guarantees congestion freedom since the
sum of the $MG$ does not exceed the link bandwidth.  But it can leave bandwidth
on the table.

We distribute this "remaining" bandwidth among ``needy'' VMs (those whose target
rates are more than their guaranteed rates), in proportion to their weights.
This process must be iterative, since we may end up allocating a VM more than
its target rate calculated in the first phase.  Since the target rate is our
proxy for what the VM would desire in an ideal world without sharing, we remove
this bandwidth and iterate again.  Note also that the process is guaranteed to
terminate since at least one VM will be removed from the needy list in each
iteration. In practice, the loop terminates within a few nanoseconds, even for
100s of VMs.
