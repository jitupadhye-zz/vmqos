\section{MBFQ Algorithm}
\label{sec:algorithm}

\begin{figure}[t]
{\footnotesize
\begin{algorithmic}
\State // later ...
\end{algorithmic}
}
\caption{MBFQ notation and initialization}
\label{fig:mbfq_init}
\end{figure}

\begin{figure}[t]
{\footnotesize
\begin{algorithmic}
\State // later ...
\end{algorithmic}
}
\caption{MBFQ Phase 1}
\label{fig:mbfq_p1}
\end{figure}

\begin{figure}[t]
{\footnotesize
\begin{algorithmic}
\State // later ...
\end{algorithmic}
}
\caption{MBFQ Phase 2}
\label{fig:mbfq_p2}
\end{figure}

The MBFQ algorithm calculates rate allocation in two steps every $T$ seconds.
First, we compute an ideal target rate ($TR$) for each VM, by comparing its
measured send rate ($SR$) to the allocated rate ($AR$). $TR$ is a proxy for what
the VM would desire if it were not subject to constraints like link bandwidth,
and sharing with other VMs.  In the second phase, we adjust the $TR$ to ensure
that the link is not oversubscribed and rates are allocated according to
weights. These adjusted target rates then become the allocated rates.  Some
notation and initialization of MBFQ is shown in Figure~\ref{fig:mbfq_init}. 

{\bf Phase 1: Computing target rates:} 
The pseudocode for this phase is shown in Figure~\ref{fig:mbfq_p1}.  This phase
calculates a new target rate for every VM, based on its recently-measured send
rate ($SR$). Intuitively, if a VM is not fully using its allocated rate, we
should offer the residual bandwidth to other VMs.  On the other hand, if the VM
is indeed fully using its allocated rate, it may be able to use additional
bandwidth. 

We assume that a VM is fully utilizing is allocated bandwidth if $SR \geq
0.95*AR$. The 5\% margin allows for small, unavoidable variations in the sending
rate. To increase the bandwidth allocated to such a VM we need to strike a
balance between two competing goals. First, we must minimize ``waste''-- just
because we give additional bandwidth to a VM does not mean it will be able to
use it. Second, a customers must be able to quickly ramp up to the bandwidth
that they have paid for, if they are backlogged.

Our approach works as follows.  If, for a given VM, $SR \geq 0.95*AR$, we set
$TR = 1.2*AR$.  If the VM qualifies again in the immediate next round, we set
$TR = 1.5*AR$, and if it qualifies again, we set $TR = \max(2*AR, MG)$ -- i.e.
if needed, we let it go to full minimum guaranteed bandwidth (or higher).  Thus,
we are conservative in the first two rounds, but a customer with substantial
pending demand is guaranteed to reach the minimum guaranteed bandwidth in three
time intervals or less.

If, on the other hand, the VM is using less than 85\% of its allocated
bandwidth, i.e. $SR \leq 0.85*AR$ for ten consecutive intervals (i.e. 1 second),
we reduce the bandwidth allocated to this VM by setting $TR = 1.1*SR$. 

{\bf Phase 2:  Preventing Congestion and Enforcing Fair Sharing:} The pseudocode
for this phase is shown in Figure~\ref{fig:mbfq_p2}. Our goal is to adjust the
target rates so that the link is not oversubscribed, and any leftover bandwidth
is allocated in proportion to the VM's weights.

We start by initializing each VM's allocated rate to be the minimum of its
target rate and its guaranteed rates.  This initial allocation guarantees
congestion freedom since the sum of the guaranteed rates does not exceed the
link bandwidth.  But it can leave bandwidth on the table.

To distribute this "remaining" bandwidth, we mark the VMs whose target rates are
more than their guaranteed rates as VMs that "need more bandwidth".  We allocate
the remaining bandwidth to these needy VMs in proportion to their weights.  Note
that this process must be iterative, since we may end up allocating a VM more
than its target rate calculated in the first phase.  Since the target rate is
our proxy for what the VM would desire in an ideal world without sharing, we
remove this bandwidth and iterate again.  Note also that the process is
guaranteed to terminate since at least one VM will be removed from the needy
list in each iteration. In practice, the loop terminates within a few
nanoseconds, even for 100s of VMs.

